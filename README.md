# Rehab\_RAG: 理学療法ガイドラインのためのRAG手法評価・検証リポジトリ

[![Project Status: Experimental](https://img.shields.io/badge/status-experimental-orange)](https://github.com/YouSayH/Rehab_RAG)

## 概要 (Overview)

このリポジトリは、理学療法（リハビリテーション）領域の臨床ガイドラインなどの専門文書に対して、最適な**RAG (Retrieval-Augmented Generation)** パイプラインを模索・評価するために作成されました。

最終的な目標は、臨床現場の専門家が持つ疑問に対し、膨大なガイドラインの中から、迅速かつ正確に、そして根拠を持って回答を提示できるAIアシスタントを構築することです。

そのために、様々なRAGの構成要素（チャンキング、クエリ拡張、フィルタリング等）を組み合わせた手法を個別のディレクトリで実装し、それぞれの性能（精度、速度、コスト）を比較・検証します。

## RAG手法の構成要素 (Components of RAG Techniques)

このリポジトリでは、以下の構成要素を様々に組み合わせてRAGパイプラインを構築・評価します。

1.  **チャンキング戦略 (Chunking Strategies)**
    *   **概要**: 知識源となる長文のドキュメントを、LLMが扱いやすいサイズのかたまり（チャンク）に分割する手法。
    *   **検証中の手法**:
        *   `structured_semantic_chunk`: Markdownの構造（見出し）を維持しつつ、意味のある段落単位で分割するハイブリッド戦略。

2.  **ベクトルDBとEmbeddingモデル (Vector DB & Embeddings)**
    *   **概要**: テキストチャンクを意味を捉えたベクトル（Embedding）に変換し、高速な類似度検索を可能にするデータベース。
    *   **検証中の手法**:
        *   `chromadb`: ローカル環境で手軽に構築できるベクトルデータベース。
        *   `sentence-transformers`: 多言語対応など、様々な特性を持つEmbeddingモデルライブラリ。(`intfloat/multilingual-e5-large`などを使用)

3.  **クエリ拡張 (Query Expansion)**
    *   **概要**: ユーザーの短い質問文だけでは検索精度が上がらない場合に、質問文をより検索に適した形に変換・拡張する手法。
    *   **検証中の手法**:
        *   `hyde_prf` (Hypothetical Document Embeddings / Pseudo Relevance Feedback): ユーザーの質問からLLMが「架空の理想的な回答」を生成し、その回答文でベクトル検索を行うことで、検索精度を向上させる。

4.  **プロンプト構築 (Prompt Construction)**
    *   **概要**: 検索で得られた文書チャンクを、LLMへの指示（プロンプト）にどのように組み込むかという戦略。
    *   **検証中の手法**:
        *   `retrieval-injected`: 検索結果を「参考情報」としてプロンプトに明記し、LLMにその情報のみに基づいて回答するよう厳密に指示する。

5.  **後処理とフィルタリング (Post-processing & Filtering)**
    *   **概要**: 検索で得られた文書チャンクの中には、ノイズや質問と矛盾する内容が含まれる場合があります。これらを最終的な回答生成前に除去する手法。
    *   **検証中の手法**:
        *   `nli_filter` (Natural Language Inference): NLIモデルを使い、検索結果と元の質問文が「矛盾」していないかを判定し、矛盾する情報をフィルタリングする。




## 📂 ディレクトリ構成

このプロジェクトは、RAGの各コンポーネントをモジュールとして管理し、それらを柔軟に組み合わせて実験できるよう設計されています。

```

Rehab\_RAG/
│
├── 📁 source\_documents/
│   └── 📄 脳卒中理学療法\_ガイドライン.md
│
├── 📁 rag\_components/
│   ├── 📁 chunkers/
│   ├── 📁 embedders/
│   ├── 📁 query\_enhancers/
│   ├── 📁 retrievers/
│   ├── 📁 rerankers/
│   ├── 📁 filters/
│   └── 📁 llms/
│
├── 📁 experiments/
│   └── 📁 \<実験名\>/
│       ├── 📜 config.yaml
│       ├── 🐍 build\_database.py
│       ├── 🐍 query\_rag.py
│       └── 🗃️ db/
│
├── 🔑 .env
└── 📋 requirements.txt

```

---

### ### 各要素の詳細

#### `📁 source_documents/`
**RAGシステムの知識源となるMarkdownファイル**を格納します。ここに置かれた文書がチャンキング（分割）され、ベクトルデータベースに登録されます。

#### `📁 rag_components/`
RAGパイプラインを構成する各機能（チャンカー、エンベッダー等）が、**再利用可能なPythonクラスとしてモジュール化**されています。新しい手法を試す際は、このディレクトリに新しいコンポーネントを追加します。
* `chunkers/`: テキストを意味のある塊（チャンク）に分割するロジック。
* `embedders/`: テキストをベクトルに変換するモデルのラッパー。
* `query_enhancers/`: ユーザーの質問を検索用に拡張・変換する処理。
* `retrievers/`: ベクトルDBとのやり取り（保存・検索）を担当。
* `rerankers/`: 検索結果をより精度の高いモデルで並べ替える処理。
* `filters/`: 検索結果からノイズや矛盾する情報を除去する処理。
* `llms/`: 回答生成モデル（Geminiなど）とのAPI通信を管理するラッパー。

#### `📁 experiments/`
様々なRAGコンポーネントの組み合わせ（＝パイプライン）を試すための**実験室**です。各サブディレクトリが、それぞれ独立した一つの実験設定に対応します。

* `📁 <実験名>/`: ディレクトリ名は、その実験で採用している手法の組み合わせを表しています。
    * `📜 config.yaml`: その実験で使用する**コンポーнентやモデルを指定する、最も重要な設定ファイル**です。このファイルを編集するだけで、パイプラインの挙動を切り替えることができます。
    * `🐍 build_database.py`: `config.yaml`の設定に従い、`source_documents`から文書を読み込み、ベクトルDBを構築するスクリプトです。
    * `🐍 query_rag.py`: `config.yaml`で定義されたパイプラインを使い、ユーザーからの質問に応答する対話型スクリプトです。
    * `🗃️ db/`: `build_database.py`によって構築されたChromaDBのデータが保存される場所です。

#### `🔑 .env`
Geminiなどの**APIキーを安全に管理するためのファイル**です。`gitignore`により、このファイルはGitの追跡対象外となり、誤って公開されるのを防ぎます。

#### `📋 requirements.txt`
プロジェクトで必要な**Pythonライブラリとそのバージョンを定義したファイル**です。`pip install -r requirements.txt`コマンドで環境を簡単に再現できます。


## セットアップ (Setup)

### 1. リポジトリのクローン

```bash
git clone https://github.com/YouSayH/Rehab_RAG.git
cd Rehab_RAG
```

### 2. 仮想環境の作成と有効化

```bash
python -m venv venv
# Windows
.\venv\Scripts\activate
# Mac/Linux
source venv/bin/activate
```

### 3. 必要なライブラリのインストール

各手法のディレクトリに移動し、`requirements.txt`（もしあれば）を使ってインストールしてください。なければ、以下の主要なライブラリをインストールします。

```bash
pip install -r requirements.txt
```

### 4. APIキーの設定

プロジェクトのルートディレクトリに `.env` という名前のファイルを作成し、お使いのGoogle AI (Gemini) のAPIキーを記述します。

``` .env
GEMINI_API_KEY="ここにあなたのAPIキーを貼り付けてください"
```

### 5. 知識ソースの配置

`source_documents` ディレクトリに、知識ベースとしたいMarkdownファイルを配置してください。

## 使い方 (Usage)

各手法は、以下の2ステップで実行します。

1.  **データベースの構築**: `source_documents` の内容をベクトル化します。
2.  **RAGパイプラインの実行**: 対話形式で質問応答を行います。

### 実行例

試したい手法のディレクトリに移動して、以下のコマンドを実行します。

```bash
# (例) 今回の手法のディレクトリに移動
cd structured_semantic_chunk-hyde_prf-chromadb_sentencetransformers-nli_filter

# 1. データベースの構築（初回のみ、または文書更新時に実行）
python build_database.py

# 2. RAGパイプLINEの実行
python query_rag.py
```

## 🗺️ 今後の検証ロードマップ (Future Validation Roadmap)

このプロジェクトでは、以下のステップで段階的にRAGパイプラインを高度化させていきます。各ステップは独立した実験として実装・評価し、その知見を積み上げていくことを目的とします。

---

### STEP 1: ハイブリッド検索 (Hybrid Search) の導入

* **手法の概要**: 従来のキーワード検索（Sparse Retrieval, 例: BM25）と、意味の近さで検索するベクトル検索（Dense Retrieval）を組み合わせる手法です。
* **目的と効果**: ベクトル検索が苦手な**固有名詞や専門用語の検索漏れを防ぎ**つつ、キーワード検索では捉えきれない**文脈や意図を理解する**ことができます。両者の長所を活かすことで、検索精度を大幅に向上させることが期待できます。これはRAGの土台を強化する、非常に費用対効果の高い改善です。
* **実装アプローチ**:
    1.  `rank_bm25` などのライブラリを用いて、キーワード検索のスコアを算出するコンポーネントを追加します。
    2.  ChromaDBからのベクトル検索スコアとBM25のスコアを、Reciprocal Rank Fusion (RRF) などのアルゴリズムで統合し、最終的なランキングを決定するロジックをRetrieverに組み込みます。

---

### STEP 2: 複数クエリ生成 (Multi-Query Retriever)

* **手法の概要**: ユーザーからの1つの複雑な質問を、LLMを使って複数の異なる観点を持つサブ質問に分解し、それぞれのサブ質問で検索を実行する手法です。
* **目的と効果**: 「AとBの違いと、その歴史的背景について」のような多角的な質問に対して、**情報の取りこぼしをなくし、網羅性を高めます**。1回の検索では得られにくい、多様な側面からの情報を収集することが可能になります。
* **実装アプローチ**:
    1.  Query Enhancerの新しい手法として実装します。
    2.  ユーザーの質問をLLMに渡し、「この質問に多角的に答えるために必要な、3つの検索クエリを生成してください」といったプロンプトを実行します。
    3.  生成された各クエリでRetrieverを実行し、得られた文書群を統合・重複排除して後段のRerankerに渡します。

---

### STEP 3: RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval)

* **手法の概要**: 文書群を再帰的にクラスタリングし、各クラスタの要約を作成することで、知識を**階層的なツリー構造**で表現する先進的なインデックス化手法です。
* **目的と効果**: ユーザーの質問の**抽象度に応じて、最適な粒度の情報を提供**できるようになります。例えば、「脳卒中リハビリの全体像は？」という抽象的な質問にはツリーの上位にある要約を、「特定の運動療法の詳細は？」という具体的な質問には末端のチャンクを提供できます。これにより、LLMはより質の高い回答を生成できます。
* **実装アプローチ**:
    1.  `build_database.py`に新しいインデックス構築ロジックとして追加します。
    2.  全チャンクをクラスタリングし、各クラスタをLLMで要約します。
    3.  生成された要約を新たなチャンクとみなし、再度クラスタリングと要約を繰り返してツリーを構築します。
    4.  検索時は、このツリー構造を横断的に探索するRetrieverを実装します。

---

### STEP 4: Self-RAG (Self-Reflective RAG)

* **手法の概要**: LLM自身が「検索は必要か？」「検索結果は質問に関連しているか？」といった**自己評価（Reflection）を行いながら、動的に思考プロセスを制御**するフレームワークです。
* **目的と効果**: パイプラインに**自律性**をもたらし、不要な検索をスキップしたり、不適切な情報を自己判断で破棄したりできます。これにより、プロセスの効率化と、ハルシネーション（事実に基づかない情報の生成）の抑制が期待できます。
* **実装アプローチ**:
    1.  `query_rag.py`のメインループを改修します。
    2.  LLMへのプロンプトを工夫し、思考のステップとして「検索判断トークン」や「関連性評価トークン」を生成させます。
    3.  生成されたトークンの内容に応じて、検索を実行するか、どの文書を利用するかを決定する条件分岐ロジックを組み込みます。

---

### STEP 5: GraphRAG (ナレッジグラフの活用)

* **手法の概要**: 文書からエンティティ（例: 疾患、治療法）とそれらの関係性を抽出し、**知識をグラフ構造（ナレッジグラフ）として表現**します。検索時は、このグラフ内の関連する部分グラフを探索します。
* **目的と効果**: 文書単体では見えにくい、**エンティティ間の複雑な関係性を捉えた回答**が可能になります。「治療法Aと治療法Bは、どちらも症状Cに効果があるとされるが、その作用機序の違いは何か？」といった、高度な推論を要する質問に強くなります。これはプロジェクトの最終目標の一つです。
* **実装アプローチ**:
    1.  **インデックス構築**: LLMを用いて全文書からエンティティと関係（`(主語, 関係, 目的語)`のトリプル）を抽出し、`Neo4j`などのグラフデータベースに格納します。
    2.  **検索**: ユーザーの質問をLLMでグラフクエリ（例: Cypher）に変換します。
    3.  **情報統合**: グラフデータベースから取得したサブグラフ（関連情報）をテキスト形式に変換（線形化）し、最終的なコンテキストとしてLLMに渡して回答を生成させます。